{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.24.9)\n",
      "Requirement already satisfied: python-docx in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: openai==0.28 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (3.9.5)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.9 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymupdf) (1.24.9)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (5.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf python-docx openai==0.28 numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import docx\n",
    "import fitz\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OpenAI API Key not found. Please set it in the .env file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_from_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def load_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = [para.text for para in doc.paragraphs]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def load_text_from_pdf(file_path):\n",
    "    pdf = fitz.open(file_path)\n",
    "    text = [pdf.load_page(page_num).get_text() for page_num in range(len(pdf))]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('\\n', ' ')  # Remove newlines\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to vectorize text\n",
    "def vectorize_text(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Function to search text using cosine similarity\n",
    "def search_text(query, text):\n",
    "    query_vector = vectorize_text(query)\n",
    "    text_vector = vectorize_text(text)\n",
    "    similarity = cosine_similarity([query_vector], [text_vector])[0][0]\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate text using the OpenAI Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(retrieved_text, user_input):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI specialized in answering questions based on provided text data. Limit your response to the context of the provided text.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "            {\"role\": \"assistant\", \"content\": retrieved_text},\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0]['message']['content'].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to handle the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_application(file_path, query):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".txt\":\n",
    "        text = load_text_from_txt(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        text = load_text_from_docx(file_path)\n",
    "    elif ext == \".pdf\":\n",
    "        text = load_text_from_pdf(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    # Preprocess the loaded text\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    similarity = search_text(query, text)\n",
    "\n",
    "    if similarity > 0.5:\n",
    "        generated_text = generate_text(text, query)\n",
    "        return generated_text\n",
    "    else:\n",
    "        return \"No relevant information found in the text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process the query and return the result\n",
    "def rag_generate(query):\n",
    "    result = rag_application(file_path, query)\n",
    "    return result\n",
    "\n",
    "# Set the file path (update this path to your file's location)\n",
    "file_path = \"C:\\\\Users\\\\pc\\\\OneDrive\\\\Bureau\\\\ragapprroject\\\\NLP models.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the subject of this text?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The subject of this text is a research report on pre-trained NLP models, focusing on their significance in advancing machine learning operations and software development life cycles. It explores various pre-trained models in NLP, such as BERT, GPT-3, ELMo, Transformer-XL, and RoBERTa, along with innovative approaches like Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). The report delves into the architecture, training processes, challenges, and improvements associated with these pre-trained models, highlighting their transformative potential in shaping the future of AI applications.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_generate(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP stands for Natural Language Processing. It involves the use of pre-trained models and language models to enhance the efficiency and effectiveness of tasks related to understanding and processing human language. Pre-trained models are deep learning models that have been trained on large amounts of data before being fine-tuned for specific tasks in NLP, such as language translation, sentiment analysis, and text summarization. Some of the best pre-trained models in NLP include BERT (Bidirectional Encoder Representations from Transformers), GPT-3 (Generative Pretrained Transformer 3), ELMo (Embeddings from Language Models), Transformer-XL, and RoBERTa (Robustly Optimized BERT). These models are used for various NLP tasks due to their superior performance and resource-saving capabilities.\n",
      "\n",
      "\n",
      "Pre-training in the context of the provided text refers to training deep learning models on extensive datasets before fine-tuning them for specific tasks. Pre-training allows these models to learn patterns and features in natural language processing (NLP) tasks, serving as a starting point for tasks such as translation, sentiment analysis, and summarization. The pre-training phase involves unsupervised learning on large text corpora, enabling the model to understand and generate coherent text. After pre-training, the models can be fine-tuned on smaller labeled datasets to adapt them to specific tasks, such as chatbot development. Fine-tuning and reinforcement learning from human feedback are crucial steps to tailor the model for real-world applications and improve its performance.\n",
      "\n",
      "\n",
      "Some challenges with large language models (LLMs) include:\n",
      "\n",
      "1. Outdated Training Data: LLMs may suffer from using outdated training data, limiting their ability to generate accurate and relevant responses, especially for recent events or evolving trends.\n",
      "\n",
      "2. Hallucination: LLMs might confidently generate plausible-sounding but false information due to gaps in their knowledge, a phenomenon known as \"hallucination.\"\n",
      "\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a method introduced by Meta AI researchers that combines an information retrieval component with a text generator model to address knowledge-intensive tasks and enhance the capabilities of Large Language Models (LLMs). RAG addresses the limitations of static training data by allowing LLMs to access and integrate up-to-date information from external knowledge sources in real-time. The components of RAG include an orchestration layer, retrieval tools, LLMs, and operational flow, which work together to manage user inputs, retrieve relevant information, and generate contextually appropriate responses. RAG improves performance by emphasizing clean and relevant data inputs and optimization strategies, such as tuning text chunk sizes and experimenting with different embedding models. Compared to fine-tuning, RAG provides a mechanism to update and adapt LLM knowledge dynamically without the need for retraining, allowing LLMs to stay current with evolving information and trends.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"What is NLP?\", \"How does Pre-training work?\",\"What are Challenges with large language models (LLMs)?\",\"What is Retrieval augmented generation (RAG)?\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = rag_generate(prompt)\n",
    "    print(response)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
