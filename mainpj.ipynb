{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.24.9)\n",
      "Requirement already satisfied: python-docx in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: openai==0.28 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (3.9.5)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.9 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymupdf) (1.24.9)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (5.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf python-docx openai==0.28 numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import docx\n",
    "import fitz\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OpenAI API Key not found. Please set it in the .env file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path: str) -> str:\n",
    "    \"\"\"Load text from various file types.\"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".txt\":\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    elif ext == \".docx\":\n",
    "        doc = docx.Document(file_path)\n",
    "        return '\\n'.join(para.text for para in doc.paragraphs)\n",
    "    elif ext == \".pdf\":\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            return '\\n'.join(page.get_text() for page in pdf)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    return text.lower().replace('\\n', ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text: str) -> np.ndarray:\n",
    "    \"\"\"Create a vector representation of the text.\"\"\"\n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        return np.array(response['data'][0]['embedding'])\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"Error in vectorizing text: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to chunk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk text\n",
    "def chunk_text(text: str, chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"Split text into chunks of specified size.\"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to find the most relevant chunk\n",
    "def find_most_relevant_chunk(query: str, chunks: List[str]) -> Tuple[str, float]:\n",
    "    \"\"\"Find the most relevant chunk to the query.\"\"\"\n",
    "    query_vector = vectorize_text(query)\n",
    "    if query_vector is None:\n",
    "        return \"\", 0.0\n",
    "\n",
    "    chunk_vectors = [vectorize_text(chunk) for chunk in chunks]\n",
    "    valid_chunk_vectors = [vec for vec in chunk_vectors if vec is not None]\n",
    "\n",
    "    if not valid_chunk_vectors:\n",
    "        return \"\", 0.0\n",
    "\n",
    "    similarities = cosine_similarity([query_vector], valid_chunk_vectors)[0]\n",
    "    max_similarity_index = np.argmax(similarities)\n",
    "    return chunks[max_similarity_index], similarities[max_similarity_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate text using the OpenAI Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response\n",
    "def generate_response(context: str, query: str) -> str:\n",
    "    \"\"\"Generate a response using OpenAI's API.\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI specialized in answering questions based on provided text data. Limit your response to the context of the provided text.\"},\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "                {\"role\": \"assistant\", \"content\": context},\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0]['message']['content'].strip()\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"Error in generating response: {e}\")\n",
    "        return \"Sorry, I couldn't generate a response at this time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to handle the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_application(file_path: str, query: str, similarity_threshold: float = 0.5) -> str:\n",
    "    \"\"\"Main RAG application function.\"\"\"\n",
    "    try:\n",
    "        text = load_text(file_path)\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        chunks = chunk_text(preprocessed_text)\n",
    "        most_relevant_chunk, similarity = find_most_relevant_chunk(query, chunks)\n",
    "        \n",
    "        if similarity > similarity_threshold:\n",
    "            return generate_response(most_relevant_chunk, query)\n",
    "        else:\n",
    "            return \"No relevant information found in the text.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG application: {e}\")\n",
    "        return \"An error occurred while processing your request.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path (update this path to your file's location)\n",
    "file_path = \"C:\\\\Users\\\\pc\\\\OneDrive\\\\Bureau\\\\ragapprroject\\\\NLP models.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the subject of this text?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The subject of the text is about the implementation of fine-tuning and reinforcement learning from human feedback in order to adapt models for real-world applications like chatbots. The text discusses pre-training objectives, data collection, data cleaning, and tokenization as part of this implementation process.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_application(file_path, prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is NLP?\n",
      "Response: NLP stands for Natural Language Processing. It involves the use of computational techniques to analyze, understand, and generate human language data. In the context provided, NLP is discussed in relation to pre-trained models that enhance performance, scalability, and innovation in AI applications. The text suggests that leveraging these models can lead to advancements in AI development and deployment.\n",
      "\n",
      "Query: How does Pre-training work?\n",
      "Response: Pre-training works by training a model to predict the next word in a sequence of text. This initial training helps the model understand language patterns and concepts from a diverse set of text data. The collected data is cleaned to remove noise and inconsistencies, and then tokenized to break down the text into smaller units for processing. This pre-training process forms the foundation for the model to understand language, but further steps like fine-tuning and reinforcement learning are necessary to adapt the model for specific tasks such as chatbots.\n",
      "\n",
      "Query: What are Challenges with large language models (LLMs)?\n",
      "Response: Some challenges with large language models (LLMs) include:\n",
      "\n",
      "1. Training complexity: LLMs require extensive training on diverse datasets to learn various linguistic patterns and knowledge, which can be computationally intensive and time-consuming.\n",
      "\n",
      "2. Fine-tuning: Adapting pre-trained LLMs to specific tasks or domains may require additional fine-tuning, which can be challenging due to the complexity of these models.\n",
      "\n",
      "3. Interpretability: LLMs are often criticized for their lack of interpretability, making it difficult to understand how they arrive at their predictions or generate text.\n",
      "\n",
      "4. Bias and ethical concerns: LLMs can amplify biases present in the training data, leading to potential ethical concerns related to fairness and inclusivity in AI applications.\n",
      "\n",
      "5. Resource requirements: LLMs are resource-intensive, requiring significant computational power and data to train and deploy effectively.\n",
      "\n",
      "These challenges highlight the importance of addressing issues related to training, fine-tuning, interpretability, bias, and resource allocation when working with\n",
      "\n",
      "Query: What is Retrieval augmented generation (RAG)?\n",
      "Response: Retrieval Augmented Generation (RAG) is a method introduced by Meta AI researchers that combines an information retrieval component with a text generator model to enhance the capabilities of large language models (LLMs). It addresses the limitations of static training data by allowing LLMs to access and integrate up-to-date information from external knowledge sources in real-time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"What is NLP?\", \"How does Pre-training work?\",\"What are Challenges with large language models (LLMs)?\",\"What is Retrieval augmented generation (RAG)?\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = rag_application(file_path, prompt)\n",
    "    print(f\"Query: {prompt}\\nResponse: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
