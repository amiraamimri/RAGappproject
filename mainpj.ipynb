{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf python-docx openai==0.28 numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import docx\n",
    "import fitz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OpenAI API Key not found. Please set it in the .env file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_from_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def load_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = [para.text for para in doc.paragraphs]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def load_text_from_pdf(file_path):\n",
    "    pdf = fitz.open(file_path)\n",
    "    text = [pdf.load_page(page_num).get_text() for page_num in range(len(pdf))]\n",
    "    return '\\n'.join(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    \"\"\"\n",
    "    Convert text into a vector using OpenAI's embedding model.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be vectorized.\n",
    "\n",
    "    Returns:\n",
    "    np.array: A numpy array containing the text embeddings.\n",
    "    \"\"\"\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def search_text(query, text):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between the vectorized query and the vectorized text.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The user's query.\n",
    "    text (str): The text to be searched.\n",
    "\n",
    "    Returns:\n",
    "    float: The similarity score between the query and the text.\n",
    "    \"\"\"\n",
    "    query_vector = vectorize_text(query)\n",
    "    text_vector = vectorize_text(text)\n",
    "    similarity = np.dot(text_vector, query_vector)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate text using the OpenAI Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(retrieved_text, user_input):\n",
    "    \"\"\"\n",
    "    Generate a response based on retrieved text and user input using OpenAI's GPT-3.5 Turbo.\n",
    "\n",
    "    Parameters:\n",
    "    retrieved_text (str): The relevant text retrieved from the document.\n",
    "    user_input (str): The user's input question.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated response text.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI specialized in answering questions based on provided text data. Limit your response to the context of the provided text.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "            {\"role\": \"assistant\", \"content\": retrieved_text},\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0]['message']['content'].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to handle the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_application(file_path, query):\n",
    "    \"\"\"\n",
    "    Handle the Retrieval-Augmented Generation (RAG) application.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file containing the text data (.txt, .docx, or .pdf).\n",
    "    query (str): The user's query.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated text based on the query, or a message if no relevant information is found.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".txt\":\n",
    "        text = load_text_from_txt(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        text = load_text_from_docx(file_path)\n",
    "    elif ext == \".pdf\":\n",
    "        text = load_text_from_pdf(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    similarity = search_text(query, text)\n",
    "\n",
    "    if similarity > 0.5:\n",
    "        generated_text = generate_text(text, query)\n",
    "        return generated_text\n",
    "    else:\n",
    "        return \"No relevant information found in the text.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path (update this path to your file's location)\n",
    "file_path = \"C:\\\\Users\\\\pc\\\\OneDrive\\Bureau\\\\ragapprroject\\\\NLP models.pdf\"\n",
    "# Function to process the query and return the result\n",
    "def rag_generate(query):\n",
    "    \"\"\"\n",
    "    Process the query using the RAG application and return the result.\n",
    "    \n",
    "    Parameters:\n",
    "    query (str): The user's query.\n",
    "    \n",
    "    Returns:\n",
    "    str: The generated text based on the query, or a message if no relevant information is found.\n",
    "    \"\"\"\n",
    "    result = rag_application(file_path, query)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the subject of this text?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_generate(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What is NPL?\", \"How does machine learning work?\", \"What is deep learning?\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = rag_generate(prompt)\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
