{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.24.9)\n",
      "Requirement already satisfied: python-docx in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: openai==0.28 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai==0.28) (3.9.5)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.9 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymupdf) (1.24.9)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (5.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf python-docx openai==0.28 numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import docx\n",
    "import fitz\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OpenAI API Key not found. Please set it in the .env file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path: str) -> str:\n",
    "    \"\"\"Load text from various file types.\"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".txt\":\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    elif ext == \".docx\":\n",
    "        doc = docx.Document(file_path)\n",
    "        return '\\n'.join(para.text for para in doc.paragraphs)\n",
    "    elif ext == \".pdf\":\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            return '\\n'.join(page.get_text() for page in pdf)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    return text.lower().replace('\\n', ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text: str) -> np.ndarray:\n",
    "    \"\"\"Create a vector representation of the text.\"\"\"\n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        return np.array(response['data'][0]['embedding'])\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"Error in vectorizing text: {e}\")\n",
    "        return np.array([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to chunk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk text\n",
    "def chunk_text(text: str, chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"Split text into chunks of specified size.\"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to find the most relevant chunk\n",
    "def find_most_relevant_chunk(query: str, chunks: List[str]) -> Tuple[str, float]:\n",
    "    \"\"\"Find the most relevant chunk to the query.\"\"\"\n",
    "    query_vector = vectorize_text(query)\n",
    "    chunk_vectors = [vectorize_text(chunk) for chunk in chunks]\n",
    "    similarities = cosine_similarity([query_vector], chunk_vectors)[0]\n",
    "    max_similarity_index = np.argmax(similarities)\n",
    "    return chunks[max_similarity_index], similarities[max_similarity_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate text using the OpenAI Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response\n",
    "def generate_response(context: str, query: str) -> str:\n",
    "    \"\"\"Generate a response using OpenAI's API.\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI specialized in answering questions based on provided text data. Limit your response to the context of the provided text.\"},\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "                {\"role\": \"assistant\", \"content\": context},\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0]['message']['content'].strip()\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"Error in generating response: {e}\")\n",
    "        return \"Sorry, I couldn't generate a response at this time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to handle the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_application(file_path: str, query: str, similarity_threshold: float = 0.5) -> str:\n",
    "    \"\"\"Main RAG application function.\"\"\"\n",
    "    try:\n",
    "        text = load_text(file_path)\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        chunks = chunk_text(preprocessed_text)\n",
    "        most_relevant_chunk, similarity = find_most_relevant_chunk(query, chunks)\n",
    "        \n",
    "        if similarity > similarity_threshold:\n",
    "            return generate_response(most_relevant_chunk, query)\n",
    "        else:\n",
    "            return \"No relevant information found in the text.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG application: {e}\")\n",
    "        return \"An error occurred while processing your request.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path (update this path to your file's location)\n",
    "file_path = \"C:\\\\Users\\\\pc\\\\OneDrive\\\\Bureau\\\\ragapprroject\\\\NLP models.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the subject of this text?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The subject of the text appears to be about fine-tuning and reinforcement learning processes for adapting models like chatbots to real-world applications using a specific breakdown of implementation steps, including pre-training objectives, data collection, data cleaning, and tokenization.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_application(file_path, prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is NLP?\n",
      "Response: Based on the provided text, NLP stands for Natural Language Processing. It involves tasks such as text classification, machine translation, sentiment analysis, and other language-related applications. NLP leverages pre-trained models to enhance performance, scalability, and innovation in AI applications. The future of NLP involves a combination of research, practical implementation, and pushing boundaries to advance AI development and deployment.\n",
      "\n",
      "Query: How does Pre-training work?\n",
      "Response: Pre-training works by training a model to predict the next word in a sequence of text, which helps the model understand language patterns and concepts. This initial training phase is essential for the model to grasp language but doesn't enable it to comprehend specific instructions or questions. The process involves gathering a diverse set of text data, cleaning the data by removing noise and ensuring consistency, and tokenizing the text by splitting it into smaller units for processing.\n",
      "\n",
      "Query: What are Challenges with large language models (LLMs)?\n",
      "Response: Some challenges with large language models (LLMs) include:\n",
      "1. Training: Large language models require extensive training on diverse datasets to learn various linguistic patterns and knowledge.\n",
      "2. Pre-training: LLMs undergo pre-training on a large corpus of text through unsupervised learning to understand and generate coherent text.\n",
      "3. Complexity: LLMs utilize advanced transformer architectures with attention mechanisms to process input sequences, adding depth and complexity to the model's understanding.\n",
      "\n",
      "Query: What is Retrieval augmented generation (RAG)?\n",
      "Response: Retrieval Augmented Generation (RAG) is a method that combines an information retrieval component with a text generation model to address knowledge-intensive tasks and enhance the capabilities of large language models (LLMs). RAG aims to overcome limitations such as outdated training data by allowing LLMs to access and integrate up-to-date information from external knowledge sources in real-time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"What is NLP?\", \"How does Pre-training work?\",\"What are Challenges with large language models (LLMs)?\",\"What is Retrieval augmented generation (RAG)?\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = rag_application(file_path, prompt)\n",
    "    print(f\"Query: {prompt}\\nResponse: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
